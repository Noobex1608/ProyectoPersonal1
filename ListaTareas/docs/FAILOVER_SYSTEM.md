# ğŸ”„ Sistema de Respaldo AutomÃ¡tico (Failover)

## DescripciÃ³n

Sistema inteligente de respaldo que garantiza disponibilidad continua del Modo Estudio, cambiando automÃ¡ticamente entre servicios de IA segÃºn disponibilidad.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Usuario solicita contenido       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    1ï¸âƒ£ Intentar con GROQ (Primario)      â”‚
â”‚    â€¢ MÃ¡s rÃ¡pido (500 tokens/seg)        â”‚
â”‚    â€¢ Modelo: llama-3.3-70b-versatile    â”‚
â”‚    â€¢ API gratuita                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â”€â”€ âœ… Ã‰xito â†’ Retornar
               â”‚
               â””â”€â”€â”€ âŒ Error 429 / Fallo
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£ Cambiar a OLLAMA (Respaldo)         â”‚
â”‚    â€¢ IA Local (Docker)                   â”‚
â”‚    â€¢ Sin lÃ­mites                         â”‚
â”‚    â€¢ Modelo: llama3.2:3b                 â”‚
â”‚    â€¢ 100% gratis                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â””â”€â”€â”€ âœ… Retornar resultado
```

## ğŸ¯ Funciones con Respaldo

### 1. **Estudio Libre** (`studyFreeTopic`)
- **Primario**: Groq API
- **Respaldo**: Ollama local
- **Ventaja**: Explicaciones mÃ¡s rÃ¡pidas con Groq

### 2. **GeneraciÃ³n de Mapas Mentales** (`generateMindMap`)
- **Primario**: Groq API
- **Respaldo**: Python Service â†’ Ollama
- **Ventaja**: Genera diagramas complejos mÃ¡s rÃ¡pido

## ğŸš€ ConfiguraciÃ³n

### Variables de Entorno

AsegÃºrate de tener en `.env`:

```env
# Groq API (Primario)
VITE_GROQ_API_KEY=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

# Ollama (Respaldo)
VITE_OLLAMA_URL=http://localhost:11434
VITE_OLLAMA_MODEL=llama3.2:3b

# Python Service (para mapas mentales)
VITE_MINDMAP_API_URL=http://localhost:8000
```

### Servicios Necesarios

1. **Ollama (Docker)** - Siempre debe estar corriendo como respaldo:
```bash
docker-compose up -d
```

2. **Python Service** - Para mapas mentales (respaldo):
```bash
docker ps | grep agno
```

## ğŸ“Š ComparaciÃ³n de Servicios

| CaracterÃ­stica | Groq API | Ollama Local |
|---------------|----------|--------------|
| Velocidad | âš¡âš¡âš¡ Muy rÃ¡pido | âš¡âš¡ RÃ¡pido |
| LÃ­mites | 30 req/min (gratis) | â™¾ï¸ Ilimitado |
| Latencia | ~200-500ms | ~1-3s |
| Costo | Gratis (con lÃ­mites) | Gratis (sin lÃ­mites) |
| Internet | Requiere | No requiere |
| Privacidad | En la nube | 100% local |
| Modelo | llama-3.3-70b | llama3.2:3b |

## ğŸ” DetecciÃ³n de Errores

El sistema detecta automÃ¡ticamente:

### Error 429 (Groq)
```javascript
if (response.status === 429) {
  // Cambio automÃ¡tico a Ollama
  console.warn('âš ï¸ LÃ­mite de Groq alcanzado, cambiando a Ollama...')
}
```

### Otros errores
- Problemas de red
- Timeouts
- Errores de API
- Respuestas invÃ¡lidas

En todos los casos, cambia automÃ¡ticamente a Ollama.

## ğŸ“ Logs en Consola

El sistema muestra logs claros del flujo:

```
ğŸš€ Intentando generar con Groq...
âœ… Generado con Groq exitosamente

// O en caso de fallo:

ğŸš€ Intentando generar con Groq...
âš ï¸ LÃ­mite de Groq alcanzado, cambiando a Ollama...
âœ… Generado con Ollama (respaldo)
```

## ğŸ› ï¸ Archivos Modificados

### Frontend
- [`src/services/groq.ts`](../src/services/groq.ts) - Cliente Groq API
- [`src/composables/useStudyMode.ts`](../src/composables/useStudyMode.ts) - LÃ³gica de respaldo
- [`.env`](../.env) - Variables de entorno

### Backend (sin cambios necesarios)
- Python Service sigue usando Ollama

## ğŸ¨ Ventajas del Sistema

âœ… **Alta disponibilidad** - Si un servicio falla, el otro toma el control
âœ… **Sin interrupciones** - El usuario no nota el cambio
âœ… **OptimizaciÃ³n de costos** - Usa el servicio gratuito mÃ¡s rÃ¡pido primero
âœ… **Privacidad** - Respaldo local cuando sea necesario
âœ… **Escalabilidad** - FÃ¡cil agregar mÃ¡s servicios de respaldo

## ğŸ› Troubleshooting

### Groq siempre falla
```bash
# Verificar API key
echo $VITE_GROQ_API_KEY

# Probar endpoint directamente
curl -H "Authorization: Bearer gsk_..." \
     https://api.groq.com/openai/v1/models
```

### Ollama no funciona como respaldo
```bash
# Verificar que estÃ¡ corriendo
docker ps | grep ollama

# Revisar logs
docker logs lista-tareas-ollama

# Reiniciar
docker restart lista-tareas-ollama
```

### Ambos servicios fallan
1. Verifica tu conexiÃ³n a internet (para Groq)
2. Verifica que Docker estÃ© corriendo (para Ollama)
3. Revisa los logs en la consola del navegador
4. Reinicia los servicios

## ğŸ“ˆ MÃ©tricas y Monitoreo

El sistema registra en consola:
- âœ… Ã‰xitos con cada servicio
- âš ï¸ Cambios de servicio
- âŒ Errores crÃ­ticos

Para monitoreo avanzado, puedes agregar:
- Contador de requests por servicio
- Tiempo de respuesta promedio
- Tasa de fallos

## ğŸ”® PrÃ³ximas Mejoras

- [ ] Agregar mÃ¡s servicios de respaldo (Claude, OpenAI)
- [ ] Sistema de priorizaciÃ³n dinÃ¡mico
- [ ] CachÃ© de respuestas frecuentes
- [ ] MÃ©tricas de uso en dashboard
- [ ] Alertas cuando un servicio estÃ¡ caÃ­do

---

**Sistema de respaldo implementado el 17 de enero de 2026** âœ¨
